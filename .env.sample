##############################################################################
# EMAIL CONFIGURATION
##############################################################################

# Mail server password (keep secret, do NOT commit this file with real secrets)
MAIL_PASS=XXXXX

# Mail user email address
MAIL_USER=user@example.com

# Mail server hostname (SMTP/IMAP server)
MAIL_HOST=your.host


##############################################################################
# SMB CONFIGURATION
##############################################################################

# Compose profile to use: 'cpu' or 'gpu' (default: cpu)
COMPOSE_PROFILES=cpu

# Prompt template to use: 'mathematics', 'conversation', or 'programming' (default: mathematics)
PROMPT_FILE=mathematics

# Docker Compose project name used for container and network naming
PROJECT_NAME=smb


##############################################################################
# LLAMA CONFIGURATION
##############################################################################

# GPT service container name: 'gpt-cpu' or 'gpt-gpu'
GPT_SERVICE=gpt-cpu

# Batch size for inference
BATCH_SIZE=4096 

# Number of CPU threads to use (only relevant for CPU profile)
NUM_THREADS=6

# Enable GPU usage: 'true' or 'false'
USE_GPU=true

# Maximum prompt length in tokens
MAX_PROMPT_LENGTH=2048

# Maximum number of tokens to generate
MAX_TOKENS=512

# Number of layers to offload to GPU (if applicable)
GPU_LAYERS=33

# Sampling parameters for text generation
TOP_K=50
TOP_P=0.5
TEMPERATURE=0.21

# Repetition penalty to reduce repeated tokens in output
REPETITION_PENALTY=1.25

##############################################################################
## RAG CONFIG
##############################################################################

# Configuration for wikipedia RAG
WIKIPEDIA_CHUNK_SIZE=1000
WIKIPEDIA_TOP_K=5
WIKIPEDIA_MAX_CONTENT=12000
WIKIPEDIA_LANG=en
WIKIPEDIA_MIN_SECTION=200
WIKIPEDIA_BATCH_SIZE=32
WIKIPEDIA_THRESHOLD=0.5

# Configuration for the history mail embedding.
MAIL_HISTORY_SIZE=8

##############################################################################
# DOCKER HUB CONFIGURATION
##############################################################################

# Docker Hub username or registry namespace
REGISTRY_USER=user

# Docker repository name
REGISTRY_REPO=repo

# Docker image tag/version
IMAGE_TAG=first


##############################################################################
# GGUF CURATED MODELS SELECTION
#
# Uncomment the model you want to use by removing the '#' prefix
##############################################################################

# CONVERSATION MODEL
# LLM_REPO="bartowski/Mistral-7B-Instruct-v0.3-GGUF"
# LLM_NAME="Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"

# PROGRAMMING MODEL
LLM_REPO="TheBloke/CodeLlama-7B-Instruct-GGUF"
LLM_NAME="codellama-7b-instruct.Q4_K_M.gguf"

# REASONING MODEL
# LLM_REPO="TheBloke/WizardMath-7B-V1.1-GGUF"
# LLM_NAME="wizardmath-7b-v1.1.Q4_K_M.gguf"
