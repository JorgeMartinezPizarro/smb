##############################################################################
# EMAIL CONFIGURATION
##############################################################################

# Mail server password (keep secret, do NOT commit real secrets)
MAIL_PASS=XXXXX

# Mail user email address
MAIL_USER=user@example.com

# Mail server hostname (SMTP/IMAP), e.g. smtp.gmail.com
MAIL_HOST=your.host


##############################################################################
# SMB CONFIGURATION
##############################################################################

# Compose profile to use: 'cpu' or 'gpu' (default: cpu)
COMPOSE_PROFILES=cpu

# Prompt template to use: 'mathematics', 'conversation', or 'programming' (default: mathematics)
PROMPT_FILE=mathematics

# Docker Compose project name (container & network prefix)
PROJECT_NAME=smb


##############################################################################
# LLAMA CONFIGURATION
##############################################################################

# GPT service container name: 'gpt-cpu' or 'gpt-gpu'
GPT_SERVICE=gpt-cpu

# Batch size for inference (affects memory & speed)
BATCH_SIZE=4096 

# Number of CPU threads to use (only for CPU mode)
NUM_THREADS=6

# Use GPU? true to enable, false for CPU only (default: false)
USE_GPU=false

# Maximum prompt length in tokens
MAX_PROMPT_LENGTH=2048

# Maximum tokens to generate per response
MAX_TOKENS=512

# Number of layers to offload to GPU (0 disables GPU offload)
GPU_LAYERS=0

# Sampling parameters for generation
TOP_K=50
TOP_P=0.5
TEMPERATURE=0.21

# Penalty to reduce repetitive output
REPETITION_PENALTY=1.25

# Timeout for the model, in seconds

TIMEOUT=600

##############################################################################
# RAG CONFIGURATION
##############################################################################

# Wikipedia chunk size (characters)
WIKIPEDIA_CHUNK_SIZE=1000

# Number of top chunks to retrieve from Wikipedia
WIKIPEDIA_TOP_K=5

# Maximum content length to fetch from Wikipedia (characters)
WIKIPEDIA_MAX_CONTENT=12000

# Language for Wikipedia queries (ISO 639-1)
WIKIPEDIA_LANG=en

# Minimum section size in Wikipedia (characters)
WIKIPEDIA_MIN_SECTION=200

# Batch size for Wikipedia processing
WIKIPEDIA_BATCH_SIZE=32

# Similarity threshold for Wikipedia retrieval (0-1)
WIKIPEDIA_THRESHOLD=0.5

# Size of mail history to keep for embeddings
MAIL_HISTORY_SIZE=8


##############################################################################
# DOCKER HUB CONFIGURATION
##############################################################################

# Docker Hub username or registry namespace
REGISTRY_USER=user

# Docker repository name
REGISTRY_REPO=repo

# Docker image tag/version
IMAGE_TAG=first


##############################################################################
# GGUF MODELS SELECTION
#
# Uncomment the model you want to use by removing the '#' prefix
##############################################################################

# CONVERSATION MODEL
# LLM_REPO="bartowski/Mistral-7B-Instruct-v0.3-GGUF"
# LLM_NAME="Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"

# PROGRAMMING MODEL
LLM_REPO="TheBloke/CodeLlama-7B-Instruct-GGUF"
LLM_NAME="codellama-7b-instruct.Q4_K_M.gguf"

# REASONING MODEL
# LLM_REPO="TheBloke/WizardMath-7B-V1.1-GGUF"
# LLM_NAME="wizardmath-7b-v1.1.Q4_K_M.gguf"

# 20b GPT os model with good CoT
#LLM_REPO="unsloth/gpt-oss-20b-GGUF"
#LLM_NAME="gpt-oss-20b-Q4_K_M.gguf"

# CoT math expert model
#LLM_REPO="QuantFactory/MAmmoTH2-7B-Plus-GGUF"
#LLM_NAME="MAmmoTH2-7B-Plus.Q4_K_M.gguf"

# Deepseek math CoT able model
#LLM_REPO="matrixportalx/deepseek-math-7b-rl-GGUF"
#LLM_NAME="deepseek-math-7b-rl-q8_0.gguf"