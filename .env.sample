#######################################
#
# EMAIL CONFIG
#
#######################################
MAIL_PASS=XXXXX
MAIL_USER=user@example.com
MAIL_HOST=your.host
#######################################
#
# SMB CONFIG
#
#  The profile indicates whether the llama container
#  should be load on CPU or GPU. values cpu or gpu
#
#  PROMPT_FILE indicates which prompt template 
#  should be load. valid values mathematics 
#  conversation or programming
#  
#  Using distinct project names you can have more than
#  one instance of smb.
#######################################
COMPOSE_PROFILES=cpu
PROMPT_FILE=mathematics
PROJECT_NAME=smb
#######################################
#
# LLAMA CONFIG
#
# For GPU usage, you need to have:
#
#  - Docker
#  - NVIDIA Container Toolkit
#
# Compatible with WSL 
#
#######################################
GPT_SERVICE=gpt-cpu # or gpt-gpu
BATCH_SIZE=4096 
NUM_THREADS=6
USE_GPU=true # or false
MAX_PROMPT_LENGTH=2048
MAX_TOKENS=512
GPU_LAYERS=33
TOP_K=50
TOP_P=0.5
TEMPERATURE=0.21
REPETITION_PENALTY=1.25
#######################################
#
# DOCKER HUB CONFIG
#
#######################################
REGISTRY_USER=user
REGISTRY_REPO=repo
IMAGE_TAG=first
#######################################
#
#  GGUF curated models selection
#
#  Select one of the following:
#
#######################################
# CONVERSATION
#LLM_REPO="bartowski/Mistral-7B-Instruct-v0.3-GGUF"
#LLM_NAME="Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
# PROGRAMING
LLM_REPO="TheBloke/CodeLlama-7B-Instruct-GGUF"
LLM_NAME="codellama-7b-instruct.Q4_K_M.gguf"
# REASONING
#LLM_REPO="TheBloke/WizardMath-7B-V1.1-GGUF"
#LLM_NAME="wizardmath-7b-v1.1.Q4_K_M.gguf"